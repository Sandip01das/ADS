{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score\ndata = pd.read_csv('Salary.csv')\ndata.head()\ndata['Salary'] = data['Salary'].str.replace(',', '').astype(int)\nX = data['Year of Experience'].values\n\ny = data['Salary'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndef LinearRegression(X_train,y_train):\n    n=0\n    d=0 \n    m = 0\n    c = 0\n    for i in range (X_train.shape[0]):\n        \n        n = n+((X_train[i]-X_train.mean())*(y_train[i]-y_train.mean()))\n        d = d+((X_train[i]-X_train.mean())**2)\n    \n    m = n/d\n    c = y_train.mean()-(m*X_train.mean())\n    \n    return m,c\n\ndef predict(m,c,X_test):\n    y=m*X_test+c\n    return y\nm,c=LinearRegression(X_train,y_train)\nprint(m)\nprint(c)\nprint(predict(m,c,X_test[1]))\nprint(X_test[1])\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ndata = pd.read_csv('PatientDetails_Classification.csv')\n#data = pd.read_csv('diabetes.csv')\ndata=data.drop('Patient Name',axis=1)\nX = data.drop(columns='TARGET').values\n#X = data.drop(columns='Outcome').values\ny = data['TARGET'].values\n#y = data['Outcome'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n\nclass LogisticRegression:\n\n    def __init__(self, lr=0.001, n_iters=10000):\n        self.lr = lr\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n        \n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iters):\n            linear_pred = np.dot(X, self.w) + self.b\n            predictions = self.sigmoid(linear_pred)\n\n            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n            db = (1/n_samples) * np.sum(predictions - y)\n\n            self.w = self.w - self.lr * dw\n            self.b = self.b - self.lr * db\n            \n            \n    def predict(self, X):\n        linear_pred = np.dot(X, self.w) + self.b\n        y_pred = self.sigmoid(linear_pred)\n        class_pred = [1 if y > 0.5 else 0 for y in y_pred]\n        return class_pred\n\n\nscaler=MinMaxScaler()\nX_train_scaled=scaler.fit_transform(X_train)\nX_test_scaled=scaler.transform(X_test)\nmodel = LogisticRegression(lr=0.001, n_iters=10000)\nmodel.fit(X_train_scaled, y_train)\npredictions = model.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Logistic Regression Accuracy:\", accuracy)\n\ncm=confusion_matrix(y_test,predictions)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\n\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\ndata = pd.read_csv('diabetes.csv')\ndata.head()\nX = data.drop(columns='Outcome').values\ny = data['Outcome'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\ndef euclidean_distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2)**2))\n\ndef knn_predict(X_train, y_train, X_test, k):\n    pred = []\n\n    for test_point in X_test:\n        distances = [euclidean_distance(train_point, test_point) for train_point in X_train]\n        nearest_indices = np.argsort(distances)[:k]\n        nearest_labels = y_train[nearest_indices]\n        \n        # Use majority voting to determine the predicted label\n        mejvoting = Counter(nearest_labels).most_common()[0][0] ##(0,3),(1,2)\n        pred.append(mejvoting)\n    \n    return np.array(pred)\n\n\n# Calculate accuracy\ny_pred=knn_predict(X_train, y_train, X_test, k=7)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\nimport numpy as np\nimport pandas as pd\n\nclass SVM:\n    def __init__(self, learning_rate=0.01,n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                condition = y[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n                if condition:\n                    self.w -= self.lr * (2 * self.w)\n                else:\n                    self.w -= self.lr * (2*self.w - np.dot(x_i, y[idx]))\n                    self.b -= self.lr * y[idx]\n\n\n    def predict(self, X):\n        prediction = np.dot(X, self.w) + self.b\n        return np.sign(prediction).astype(int)\ndata = pd.read_csv('PatientDetails_Classification.csv')\ndata=data.drop('Patient Name',axis=1)\nX = data.drop(columns='TARGET').values\ny = data['TARGET'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclf = SVM()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_test,predictions)\nprint(accuracy)\nfrom sklearn.metrics import confusion_matrix\n\ncm=confusion_matrix(y_test,predictions) \ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#data = pd.read_csv(\"play_tennis.csv\")\ndata=pd.read_csv(\"DLBCL-2.csv\")\n#data = data.drop('day',axis=1)\nX = data.drop(columns=['target'])\ny = data['target']\n#data['target'] = data['target'].map({'DLBCL': 0, 'FL': 1})\n\ndata.head(15)\n\ndef entropy(y):\n    value_counts = y.value_counts()\n    probabilities = value_counts / len(y)\n    entropy_value = -np.sum(probabilities * np.log2(probabilities.replace(0, 1)))\n    return entropy_value\nentropy(y)\n\ndef information_gain(y, feature):\n\n    total_entropy = entropy(y)\n    \n    unique_values = feature.unique()\n    weighted_entropies = 0\n\n    for value in unique_values:\n        subset_y = y[feature == value]\n        weighted_entropies += (len(subset_y) / len(y)) * entropy(subset_y)\n\n    return total_entropy - weighted_entropies\n\nfor column in data.columns:\n    if column != 'target':\n        feature = data[column]\n        ig = information_gain(y, feature)\n        print(f\"Feature: {column}, Information Gain: {ig:.4f}\")\n\nclass Node:\n    def __init__(self, feature=None, value=None, entropy=None, information_gain=None, left=None, right=None):\n        self.feature = feature\n        self.value = value\n        self.entropy = entropy\n        self.information_gain = information_gain\n        self.left = left\n        self.right = right\n\ndef build_decision_tree(X, y):\n    if entropy(y) == 0:\n        # If all instances have the same class, create a leaf node\n        return Node(value=y.iloc[0])\n\n    if X.empty:\n        # If no features left, create a leaf node with the majority class\n        return Node(value=y.value_counts().idxmax())\n\n    # Find the best feature to split on\n    best_feature = None\n    max_info_gain = 0\n\n    for feature_name in X.columns:\n        current_info_gain = information_gain(y, X[feature_name])\n        if current_info_gain > max_info_gain:\n            max_info_gain = current_info_gain\n            best_feature = feature_name\n\n    # Create a node with the best feature\n    node = Node(feature=best_feature, entropy=entropy(y), information_gain=max_info_gain, value={})\n\n    # Recursively build the left and right subtrees\n    unique_values = X[best_feature].unique()\n    for value in unique_values:\n        subset_X = X[X[best_feature] == value].drop(columns=[best_feature])\n        subset_y = y[X[best_feature] == value]\n        child_node = build_decision_tree(subset_X, subset_y)\n\n        if node.value is None:\n            node.value = {value: child_node}\n        else:\n            node.value[value] = child_node\n\n    return node\n\ndecision_tree = build_decision_tree(X, y)\n\n\ndef predict(node, instance):\n  if node.feature is None:\n    return node.value  # Assuming 'value' holds the final class label here\n  else:\n    value = instance[node.feature]\n    if value in node.value:\n      return predict(node.value[value], instance)\n    else:\n      # Handle unseen feature values (return default class)\n      return y_train.value_counts().idxmax()  # Assuming 'y_train' holds the training target variable\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndecision_tree = build_decision_tree(X_train, y_train)\ny_pred = [predict(decision_tree, instance) for _, instance in X_test.iterrows()]\n\naccuracy_score = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy_score:.2f}\")\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint (tp,fp,fn,tn)\nacc=(tp+tn)/(tp+tn+fp+fn)\nprint (acc)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\nclass NaiveBayesClassifier:\n    def __init__(self):\n        self.class_probs = {}\n        self.feature_probs = {}\n\n    def fit(self, X, y):\n        num_samples, num_features = X.shape\n        unique_classes = np.unique(y)\n\n        for c in unique_classes:\n            # Calculate class probabilities\n            self.class_probs[c] = np.sum(y == c) / num_samples\n\n            # Calculate feature probabilities for each class\n            features_given_class = X[y == c]\n            self.feature_probs[c] = np.sum(features_given_class, axis=0) / np.sum(y == c)\n\n    def predict(self, X):\n        predictions = []\n\n        for sample in X:\n            class_scores = {}\n\n            for c, class_prob in self.class_probs.items():\n                # Calculate the probability of the sample belonging to each class\n                feature_probs_given_class = self.feature_probs[c]\n                log_prob = np.sum(np.log(sample * feature_probs_given_class ))\n                class_scores[c] = np.log(class_prob) + log_prob\n\n            # Predict the class with the highest probability\n            predicted_class = max(class_scores, key=class_scores.get)\n            predictions.append(predicted_class)\n\n        return predictions\n\ndata = pd.read_csv(\"PatientDetails_Classification.csv\")\n\n# Convert categorical features to numerical values using label encoding\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']).columns:\n    data[column] = label_encoder.fit_transform(data[column])\ndata=data.drop('Patient Name',axis=1)\nX = data.drop('TARGET', axis=1).values\ny = data['TARGET'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nnb_classifier = NaiveBayesClassifier()\nnb_classifier.fit(X_train, y_train)\npredictions = nb_classifier.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Predictions:\", predictions)\nprint(\"Accuracy:\", accuracy)\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,predictions)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef pca(X, num_components):\n\n    cov_matrix = np.cov(X, rowvar=False)\n\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Select the top 'num_components' eigenvectors\n    principal_components = eigenvectors[:, :num_components]\n\n    # Project the standardized data onto the principal components\n    transformed_data = np.dot(X, principal_components)\n\n    return transformed_data, eigenvalues, principal_components\n\ndata = pd.read_csv(\"play_tennis.csv\")\n\n# Convert categorical features to numerical values using label encoding\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']).columns:\n    data[column] = label_encoder.fit_transform(data[column])\n\nX = data.drop('play', axis=1).values\ny = data['play'].values\n# Apply PCA\nnum_components = 2\ntransformed_data, eigenvalues, principal_components = pca(X, num_components)\n\nprint(\"Original data shape:\", X.shape)\nprint(\"Transformed data shape:\", transformed_data.shape)\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Principal components:\")\nprint(principal_components)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\n\nclass RandomForest:\n    def __init__(self, n_trees=100, max_depth=10, min_samples_split=2):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            tree = DecisionTreeClassifier(max_depth=self.max_depth,min_samples_split=self.min_samples_split)\n            X_sample, y_sample = self.bootstrap_samples(X, y)\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n\n    def bootstrap_samples(self, X, y):\n    \n        n_samples = X.shape[0]\n        idxs = np.random.choice(n_samples, n_samples, replace=True)\n        X_bootstrapped = X.reset_index(drop=True).iloc[idxs]\n        y_bootstrapped = y.reset_index(drop=True).iloc[idxs]\n        return X_bootstrapped, y_bootstrapped\n\n    \n\n    def most_common_label(self, y):\n        #counter = Counter(y)\n        most_common = Counter(y).most_common(1)[0][0]\n        return most_common\n\n    def predict(self, X):\n        predictions = np.array([tree.predict(X) for tree in self.trees])\n        tree_preds = np.swapaxes(predictions, 0, 1)\n        predictions = np.array([self.most_common_label(pred) for pred in tree_preds])\n        return predictions\n\ndf = pd.read_csv('diabetes.csv')\n\nX = df.drop(columns=['Outcome'])\n\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n\nclf = RandomForest()\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint (tp,fp,fn,tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(\"accuracy is:\",accuracy)\n\nimport numpy as np\n\nweight_1 = 0\nweight_2 = 0\nbias = 0\nlearning_rate = 1\n\ndef activation(yin):\n    if yin > 0:\n        return 1\n    elif yin == 0:\n        return 0\n    else:\n        return -1\n        \nepochs = 100\n\ninput_data = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\ntargets = np.array([1, -1, -1, -1])\n\nfor epoch in range(epochs):\n    for i in range(len(input_data)):\n        yin = bias + weight_1 * input_data[i][0] + weight_2 * input_data[i][1]\n        y = activation(yin)\n        if y != targets[i]:\n            weight_1 += learning_rate * targets[i] * input_data[i][0]\n            weight_2 += learning_rate * targets[i] * input_data[i][1]\n            bias += learning_rate * targets[i]\n\nprint(\"Final weights (w1, w2):\", weight_1, weight_2)\nprint(\"Final bias:\", bias)\n\ndef predict_output(x1, x2):\n    yin = bias + weight_1 * x1 + weight_2 * x2\n    return activation(yin)\n\n# Test the trained perceptron\ntest_data = np.array([[-1, 1], [-1, -1]])\nfor x1, x2 in test_data:\n    prediction = predict_output(x1, x2)\n    print(\"Input:\", [x1, x2], \"Predicted Output:\", prediction)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef distance(point1, point2):\n  return np.sqrt(np.sum((point1 - point2) ** 2))\n\ndef kmeans(data, k, max_iterations):\n  centroids = data[np.random.choice(len(data), k, replace=False)]\n  assignments = np.zeros(len(data))\n  for _ in range(max_iterations):\n    for i, point in enumerate(data):\n      distances_to_centroids = [distance(point, centroid) for centroid in centroids]\n      assignments[i] = np.argmin(distances_to_centroids)\n\n    for cluster in range(k):\n      cluster_data = data[assignments == cluster]\n      if len(cluster_data) > 0:  # Avoid division by zero\n        centroids[cluster] = np.mean(cluster_data, axis=0)\n\n  return centroids, assignments\n\ndata = pd.read_csv(\"Iris.csv\")\n\n#features = list(data.columns)[:-1]\n#X = data[features].to_numpy()\nX = data.iloc[:, :-1].values\nk = 3\nmax_iterations = 100\n\ncentroids, assignments = kmeans(X, k, max_iterations)\n\nclustered_data = []\nfor i in range(k):\n  clustered_data.append(X[assignments == i])\n\nfor i in range(k):\n  print(f\"\\nCluster {i+1} data points:\")\n  print(data.iloc[assignments == i])\n\ncolors = ['red', 'green', 'blue'] \nfor i in range(k):\n  plt.scatter(clustered_data[i][:, 0], clustered_data[i][:, 1], c=colors[i], label=f\"Cluster {i+1}\")\n\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='black', label='Centroids')\nplt.xlabel(features[0])\nplt.ylabel(features[1])\nplt.title(f\"K-Means Clustering (k={k})\")\nplt.legend()\nplt.show()\n\n\n\nn_splits = 5\nindices = np.arange(len(X))\nnp.random.shuffle(indices)\nfold_indices = np.array_split(indices, n_splits)\nfor i in range(n_splits):\n    test_indices = fold_indices[i]\n    train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}